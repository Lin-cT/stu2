{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "comments: true\n",
    "layout: post\n",
    "title: ML CPT\n",
    "courses: { csp: {week: 26} }\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our ML Project: Titanic + CPT\n",
    "\n",
    "In this blog, we will show demonstrations of our code for the titanic machine learning model, as well as our own personalized CPT machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic ML:\n",
    "\n",
    "For the titanic project, we worked on trainnig the model with the titanic dataset. Using an API, we recieved data from the frontend, made the prediction, and sent the prediction back to the frontend to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TitanicPredictor:\n",
    "   def __init__(self):\n",
    "       self.data = None\n",
    "       self.encoder = None\n",
    "       self.model_dt = None\n",
    "       self.model_logreg = None\n",
    "       self.X_test = None\n",
    "       self.y_test = None\n",
    "      \n",
    "   def load_data(self):\n",
    "       self.data = sns.load_dataset('titanic')\n",
    "      \n",
    "   def preprocess_data(self):\n",
    "       if self.data is None:\n",
    "           raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "      \n",
    "       self.data.drop(['alive', 'who', 'adult_male', 'class', 'embark_town', 'deck'], axis=1, inplace=True)\n",
    "       self.data.dropna(inplace=True)\n",
    "       self.data['sex'] = self.data['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "       self.data['alone'] = self.data['alone'].apply(lambda x: 1 if x == True else 0)\n",
    "      \n",
    "       self.encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "       self.encoder.fit(self.data[['embarked']])\n",
    "       onehot = self.encoder.transform(self.data[['embarked']]).toarray()\n",
    "       cols = ['embarked_' + val for val in self.encoder.categories_[0]]\n",
    "       self.data[cols] = pd.DataFrame(onehot)\n",
    "       self.data.drop(['embarked'], axis=1, inplace=True)\n",
    "       self.data.dropna(inplace=True)\n",
    "      \n",
    "   def train_models(self):\n",
    "       X = self.data.drop('survived', axis=1)\n",
    "       y = self.data['survived']\n",
    "       X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "      \n",
    "       self.model_dt = DecisionTreeClassifier()\n",
    "       self.model_dt.fit(X_train, y_train)\n",
    "      \n",
    "       self.model_logreg = LogisticRegression()\n",
    "       self.model_logreg.fit(X_train, y_train)\n",
    "      \n",
    "   def evaluate_models(self):\n",
    "       if self.model_dt is None or self.model_logreg is None:\n",
    "           raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "      \n",
    "       y_pred_dt = self.model_dt.predict(self.X_test)\n",
    "       accuracy_dt = accuracy_score(self.y_test, y_pred_dt)\n",
    "       print('DecisionTreeClassifier Accuracy: {:.2%}'.format(accuracy_dt)) \n",
    "      \n",
    "       y_pred_logreg = self.model_logreg.predict(self.X_test)\n",
    "       accuracy_logreg = accuracy_score(self.y_test, y_pred_logreg)\n",
    "       print('LogisticRegression Accuracy: {:.2%}'.format(accuracy_logreg)) \n",
    "  \n",
    "   def predict_survival_probability(self, new_passenger):\n",
    "       if self.model_logreg is None:\n",
    "           raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "      \n",
    "       new_passenger['sex'] = new_passenger['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "       new_passenger['alone'] = new_passenger['alone'].apply(lambda x: 1 if x == True else 0)\n",
    "      \n",
    "       onehot = self.encoder.transform(new_passenger[['embarked']]).toarray()\n",
    "       cols = ['embarked_' + val for val in self.encoder.categories_[0]]\n",
    "       new_passenger[cols] = pd.DataFrame(onehot, index=new_passenger.index)\n",
    "       new_passenger.drop(['embarked'], axis=1, inplace=True)\n",
    "       new_passenger.drop(['name'], axis=1, inplace=True)\n",
    "      \n",
    "       dead_proba, alive_proba = np.squeeze(self.model_logreg.predict_proba(new_passenger))\n",
    "       print('Death probability: {:.2%}'.format(dead_proba)) \n",
    "       print('Survival probability: {:.2%}'.format(alive_proba)) \n",
    "       return dead_proba, alive_proba\n",
    "\n",
    "\n",
    "# Usage\n",
    "titanic_predictor = TitanicPredictor()\n",
    "titanic_predictor.load_data()\n",
    "titanic_predictor.preprocess_data()\n",
    "titanic_predictor.train_models()\n",
    "titanic_predictor.evaluate_models()\n",
    "\n",
    "\n",
    "# Define a new passenger\n",
    "passenger = pd.DataFrame({\n",
    "   'name': ['John Mortensen'],\n",
    "   'pclass': [2],\n",
    "   'sex': ['male'],\n",
    "   'age': [64],\n",
    "   'sibsp': [1],\n",
    "   'parch': [1],\n",
    "   'fare': [16.00],\n",
    "   'embarked': ['S'],\n",
    "   'alone': [False]\n",
    "})\n",
    "\n",
    "\n",
    "titanic_predictor.predict_survival_probability(passenger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Model: \n",
    "\n",
    "Data Loading and Preprocessing:\n",
    "The model begins by loading the Titanic dataset using Seaborn's load_dataset() function.\n",
    "It then preprocesses the data by dropping irrelevant columns ('alive', 'who', 'adult_male', 'class', 'embark_town', 'deck') and handling missing values.\n",
    "Categorical variables like 'sex' and 'alone' are converted into numerical format.\n",
    "\n",
    "One-Hot Encoding:\n",
    "The model uses one-hot encoding to convert the categorical variable 'embarked' into binary vectors.\n",
    "\n",
    "Model Training:\n",
    "After preprocessing, the data is split into features (X) and the target variable (y), followed by splitting into training and testing sets.\n",
    "Two models are trained: a Decision Tree Classifier (model_dt) and a Logistic Regression model (model_logreg).\n",
    "\n",
    "Model Evaluation:\n",
    "The trained models are evaluated using accuracy scores on the test data.\n",
    "\n",
    "Prediction:\n",
    "The model provides a method predict_survival_probability() to predict the survival probability of a new passenger.\n",
    "The new passenger's data is preprocessed similarly to the training data.\n",
    "The survival probability is predicted using the trained Logistic Regression model.\n",
    "\n",
    "Usage Example:\n",
    "An instance of the TitanicPredictor class is created.\n",
    "Data is loaded, preprocessed, models are trained, and then evaluated.\n",
    "A new passenger's data is defined, and the predict_survival_probability() method is called to estimate their survival probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint, jsonify, request  # jsonify creates an endpoint response object\n",
    "from flask_restful import Api, Resource # used for REST API building\n",
    "\n",
    "from model.jokes import *\n",
    "\n",
    "joke_api = Blueprint('joke_api', __name__,\n",
    "                   url_prefix='/api/jokes')\n",
    "\n",
    "# API generator https://flask-restful.readthedocs.io/en/latest/api.html#id1\n",
    "api = Api(joke_api)\n",
    "\n",
    "class TitanicAPI(Resource):\n",
    "    def post(self):\n",
    "            # Get passenger data from the API request\n",
    "            data = request.get_json()  # get the data as JSON\n",
    "            data['alone'] = str(data['alone']).lower()\n",
    "            converted_dict = {key: [value] for key, value in data.items()}\n",
    "            pass_in = pd.DataFrame(converted_dict)  # create DataFrame from JSON\n",
    "            titanic_predictor = TitanicPredictor()\n",
    "            titanic_predictor.load_data()\n",
    "            titanic_predictor.preprocess_data()\n",
    "            titanic_predictor.train_models()\n",
    "            titanic_predictor.evaluate_models()\n",
    "            dead_proba, alive_proba = titanic_predictor.predict_survival_probability(pass_in)\n",
    "            response = {\n",
    "                'dead_proba': dead_proba,  # Example probabilities, replace with actual values\n",
    "                'alive_proba': alive_proba\n",
    "            }\n",
    "            return jsonify(response)\n",
    "\n",
    "\n",
    "# Add resource to the API\n",
    "api.add_resource(TitanicAPI, '/create')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic API:\n",
    "\n",
    "The TitanicAPI class is a Flask-Restful Resource representing an endpoint of the API. It handles POST requests to the /api/jokes/create endpoint.\n",
    "In the post method, it extracts passenger data from the JSON request using request.get_json().\n",
    "The passenger data is processed and formatted, and then passed to the TitanicPredictor class (assumed to be defined elsewhere) for prediction.\n",
    "After predicting survival probabilities, a JSON response containing the probabilities is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPT ML (Depression):\n",
    "\n",
    "For our CPT project, we have decided to create a project centering around mental health and self care. We decided that we would try to find A dataset dealing with depression rates. We found a few we liked, and even turned created a dataset. We created a model to train data form the dataset to predict how likley a person would develop depression due to these factors: age, stress level, exercise, and sleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('depression_dataset.csv')\n",
    "# Split the data into features and labels\n",
    "X = data.drop('Probability of Developing Depression', axis=1)\n",
    "y = data['Probability of Developing Depression']\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Function to predict the chance of being depressed\n",
    "def predict_depression(age, stress_level, exercise_hours, sleep_hours):\n",
    "    input_data = scaler.transform([[age, stress_level, exercise_hours, sleep_hours]])\n",
    "    chance_of_depression = model.predict(input_data)[0]\n",
    "    return chance_of_depression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPT Model:\n",
    "\n",
    "Data Loading and Preparation:\n",
    "The model begins by loading a dataset containing pertinent information for predicting depression. This dataset typically comprises features such as age, stress level, exercise hours, and sleep hours, alongside a target variable indicating the likelihood of developing depression.\n",
    "Subsequently, the model divides the data into features (X) and the target variable (y). Features represent the input variables utilized for predictions, while the target variable signifies what we aim to predict—in this instance, the probability of experiencing depression.\n",
    "\n",
    "Data Preprocessing:\n",
    "Before proceeding with model training, it's imperative to preprocess the data. Within this model, data preprocessing entails standardization of features using a method known as StandardScaler. Standardization ensures that all features exhibit a mean of 0 and a standard deviation of 1, thereby enhancing the efficacy of certain machine learning algorithms.\n",
    "\n",
    "Model Training:\n",
    "The model undergoes training employing a linear regression algorithm for prediction tasks. Linear regression, although straightforward, is a robust algorithm employed for establishing relationships between a dependent variable (in this scenario, the likelihood of depression) and one or more independent variables (the features).\n",
    "Training involves utilizing the preprocessed training data (X_train, y_train). Throughout this process, the model learns to discern the relationship between the input features and the target variable by minimizing the disparity between predicted values and actual observations, a process known as minimizing the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint, request, jsonify, make_response\n",
    "from flask_restful import Api, Resource\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from model.depression import *\n",
    "predict_api = Blueprint(\"predict_api\", __name__,\n",
    "                        url_prefix='/api/predict')\n",
    "api = Api(predict_api)\n",
    "# Load the dataset\n",
    "class Predict(Resource):\n",
    "    def post(self):\n",
    "        body = request.get_json()\n",
    "        age = float(body.get(\"age\"))\n",
    "        stress_level = float(body.get(\"stress\"))\n",
    "        daily_exercise_hours = float(body.get(\"exercise\"))\n",
    "        daily_sleep_hours = float(body.get(\"sleep\"))\n",
    "        chance_of_depression = predict_depression(age, stress_level, daily_exercise_hours, daily_sleep_hours)\n",
    "        chance_of_depression = max(0, min(chance_of_depression, 1))  # Ensure chance_of_depression is between 0 and 1\n",
    "        return (jsonify(f\"Based on the provided data, the chance of developing depression is: {chance_of_depression * 100:.2f}%\"))\n",
    "api.add_resource(Predict, '/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPT API:\n",
    "\n",
    "Predict Resource Class (Predict):\n",
    "This class defines the behavior for handling POST requests to the /api/predict endpoint.\n",
    "Upon receiving a POST request, it retrieves JSON data from the request body containing information such as age, stress level, daily exercise hours, and daily sleep hours.\n",
    "It then uses the predict_depression function (assumed to be defined elsewhere) to predict the chance of developing depression based on the provided input parameters.\n",
    "The predicted chance of depression is bounded between 0 and 1 using max and min functions.\n",
    "Finally, it returns a JSON response containing the predicted chance of developing depression as a percentage.\n",
    "\n",
    "JSON Response:\n",
    "The response returned by the API is a JSON object containing a string message with the predicted chance of developing depression formatted to display two decimal places."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
