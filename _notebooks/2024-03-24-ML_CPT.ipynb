{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "comments: true\n",
    "layout: post\n",
    "title: ML CPT\n",
    "courses: { csp: {week: 26} }\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our CPT ML Project\n",
    "\n",
    "For our CPT project, we have decided to create a project centering around mental health and self care. We decided that we would try to find A dataset dealing with depression rates. We found a few we liked, and even turned created a dataset. Using a tester dataset for practice, we created fake data about the chances of developing depression. We created a model to train data form the dataset to predict how likley a person would develop depression due to these factors: age, stress level, exercise, and sleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate synthetic data\n",
    "num_samples = 1000\n",
    "\n",
    "# Adjust age mean and std\n",
    "age_mean = 30                   # Mean age\n",
    "age_std = 10                    # Increased variability in age\n",
    "age = np.random.normal(age_mean, age_std, num_samples)\n",
    "\n",
    "# Adjust stress level mean and std\n",
    "stress_level_mean = 5           # Mean stress level\n",
    "stress_level_std = 2            # Increased variability in stress level\n",
    "stress_level = np.random.normal(stress_level_mean, stress_level_std, num_samples)\n",
    "\n",
    "# Adjust exercise hours mean and std\n",
    "exercise_hours_mean = 1.5       # Mean daily exercise hours\n",
    "exercise_hours_std = 0.5        # Increased variability in daily exercise hours\n",
    "exercise_hours = np.random.normal(exercise_hours_mean, exercise_hours_std, num_samples)\n",
    "\n",
    "# Adjust sleep hours mean and std\n",
    "sleep_hours_mean = 8            # Mean daily sleep hours\n",
    "sleep_hours_std = 1             # Increased variability in daily sleep hours\n",
    "sleep_hours = np.random.normal(sleep_hours_mean, sleep_hours_std, num_samples)\n",
    "\n",
    "# Calculate probability of developing depression based on the factors\n",
    "probability = np.maximum(0, (age - age_mean) + \\\n",
    "              (stress_level - stress_level_mean) + (1.5 - exercise_hours) + \\\n",
    "              (8 - sleep_hours))\n",
    "\n",
    "# Create DataFrame without 'Depression' column\n",
    "data = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Stress Level': stress_level,\n",
    "    'Daily Exercise Hours': exercise_hours,\n",
    "    'Daily Sleep Hours': sleep_hours,\n",
    "    'Probability of Developing Depression': probability\n",
    "})\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "data.to_csv('depression_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DepressionPredictor:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.model_logreg = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.scaler = None\n",
    "      \n",
    "    def load_data(self, filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "      \n",
    "    def preprocess_data(self):\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "      \n",
    "        # Any necessary preprocessing steps can be added here\n",
    "\n",
    "        # For example, dropping columns, handling missing values, encoding categorical variables, etc.\n",
    "      \n",
    "    def train_models(self):\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "        \n",
    "        X = self.data.drop('Depression', axis=1)\n",
    "        y = self.data['Depression']\n",
    "        X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "      \n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "      \n",
    "        self.model_logreg = LogisticRegression()\n",
    "        self.model_logreg.fit(X_train_scaled, y_train)\n",
    "      \n",
    "    def evaluate_models(self):\n",
    "        if self.model_logreg is None:\n",
    "            raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "      \n",
    "        y_pred_logreg = self.model_logreg.predict(self.X_test)\n",
    "        accuracy_logreg = accuracy_score(self.y_test, y_pred_logreg)\n",
    "        print('LogisticRegression Accuracy: {:.2%}'.format(accuracy_logreg)) \n",
    "  \n",
    "    def predict_depression_probability(self, new_data):\n",
    "    if self.model_logreg is None:\n",
    "        raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "\n",
    "    # Preprocess new data similarly to training data\n",
    "    new_data_processed = new_data.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    new_data_processed['Family History of Depression'] = new_data_processed['family_history'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    # Add any additional preprocessing steps here\n",
    "\n",
    "    # Ensure consistency in feature names and order\n",
    "    new_data_processed = new_data_processed.rename(columns={'age': 'Age',\n",
    "                                                            'exercise_hours': 'Daily Exercise Hours',\n",
    "                                                            'family_history': 'Family History of Depression',\n",
    "                                                            'sleep_hours': 'Daily Sleep Hours',\n",
    "                                                            'stress_level': 'Stress Level'})\n",
    "\n",
    "    # Drop duplicate 'Family History of Depression' column if present\n",
    "    if 'Family History of Depression' in new_data_processed.columns:\n",
    "        new_data_processed = new_data_processed.drop('Family History of Depression', axis=1)\n",
    "\n",
    "    # Check if 'Probability of Developing Depression' column is present\n",
    "    if 'Probability of Developing Depression' in new_data_processed.columns:\n",
    "        new_data_processed = new_data_processed.drop('Probability of Developing Depression', axis=1)\n",
    "\n",
    "    # Debug: Print feature names in the new data\n",
    "    print(\"Feature names in new data:\", new_data_processed.columns)\n",
    "\n",
    "    # Ensure that feature names match those seen during fit time\n",
    "    expected_feature_names = set(['Age', 'Daily Exercise Hours', 'Family History of Depression', 'Daily Sleep Hours', 'Stress Level'])\n",
    "    new_feature_names = set(new_data_processed.columns)\n",
    "    if expected_feature_names != new_feature_names:\n",
    "        missing_features = expected_feature_names - new_feature_names\n",
    "        raise ValueError(f\"Feature names seen at fit time, yet now missing: {missing_features}\")\n",
    "\n",
    "    # Transform new data using the scaler\n",
    "    new_data_scaled = self.scaler.transform(new_data_processed)\n",
    "\n",
    "    # Predict the probability of depression\n",
    "    probability_of_depression = self.model_logreg.predict_proba(new_data_scaled)[:, 1]\n",
    "    return probability_of_depression\n",
    "\n",
    "# Usage\n",
    "depression_predictor = DepressionPredictor()\n",
    "depression_predictor.load_data('depression_dataset.csv')\n",
    "depression_predictor.preprocess_data()\n",
    "depression_predictor.train_models()\n",
    "depression_predictor.evaluate_models()\n",
    "\n",
    "# Define new data for prediction\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [30],\n",
    "    'family_history': ['Yes'],  # Assuming 'Yes' or 'No' as values\n",
    "    'stress_level': [5],\n",
    "    'exercise_hours': [1.5],\n",
    "    'sleep_hours': [8]\n",
    "})\n",
    "\n",
    "probability_of_depression = depression_predictor.predict_depression_probability(new_data)\n",
    "print('Probability of depression:', probability_of_depression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of a real dataset, we've found a few that we can use to train another model. One of the datasets provided was about students and their mwajors and how that would affect them mentally. Another one was about workers in the tech industry and how likley their circumstances would correlate with developing depression."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
