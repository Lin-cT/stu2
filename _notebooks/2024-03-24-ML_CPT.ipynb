{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "comments: true\n",
    "layout: post\n",
    "title: ML CPT\n",
    "courses: { csp: {week: 26} }\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our ML Project: Titanic + CPT\n",
    "\n",
    "In this blog, we will show demonstrations of our code for the titanic machine learning model, as well as our own personalized CPT machine learning project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic ML:\n",
    "\n",
    "For the titanic project, we worked on trainnig the model with the titanic dataset. Using an API, we recieved data from the frontend, made the prediction, and sent the prediction back to the frontend to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class TitanicPredictor:\n",
    "   def __init__(self):\n",
    "       self.data = None\n",
    "       self.encoder = None\n",
    "       self.model_dt = None\n",
    "       self.model_logreg = None\n",
    "       self.X_test = None\n",
    "       self.y_test = None\n",
    "      \n",
    "   def load_data(self):\n",
    "       self.data = sns.load_dataset('titanic')\n",
    "      \n",
    "   def preprocess_data(self):\n",
    "       if self.data is None:\n",
    "           raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "      \n",
    "       self.data.drop(['alive', 'who', 'adult_male', 'class', 'embark_town', 'deck'], axis=1, inplace=True)\n",
    "       self.data.dropna(inplace=True)\n",
    "       self.data['sex'] = self.data['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "       self.data['alone'] = self.data['alone'].apply(lambda x: 1 if x == True else 0)\n",
    "      \n",
    "       self.encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "       self.encoder.fit(self.data[['embarked']])\n",
    "       onehot = self.encoder.transform(self.data[['embarked']]).toarray()\n",
    "       cols = ['embarked_' + val for val in self.encoder.categories_[0]]\n",
    "       self.data[cols] = pd.DataFrame(onehot)\n",
    "       self.data.drop(['embarked'], axis=1, inplace=True)\n",
    "       self.data.dropna(inplace=True)\n",
    "      \n",
    "   def train_models(self):\n",
    "       X = self.data.drop('survived', axis=1)\n",
    "       y = self.data['survived']\n",
    "       X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "      \n",
    "       self.model_dt = DecisionTreeClassifier()\n",
    "       self.model_dt.fit(X_train, y_train)\n",
    "      \n",
    "       self.model_logreg = LogisticRegression()\n",
    "       self.model_logreg.fit(X_train, y_train)\n",
    "      \n",
    "   def evaluate_models(self):\n",
    "       if self.model_dt is None or self.model_logreg is None:\n",
    "           raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "      \n",
    "       y_pred_dt = self.model_dt.predict(self.X_test)\n",
    "       accuracy_dt = accuracy_score(self.y_test, y_pred_dt)\n",
    "       print('DecisionTreeClassifier Accuracy: {:.2%}'.format(accuracy_dt)) \n",
    "      \n",
    "       y_pred_logreg = self.model_logreg.predict(self.X_test)\n",
    "       accuracy_logreg = accuracy_score(self.y_test, y_pred_logreg)\n",
    "       print('LogisticRegression Accuracy: {:.2%}'.format(accuracy_logreg)) \n",
    "  \n",
    "   def predict_survival_probability(self, new_passenger):\n",
    "       if self.model_logreg is None:\n",
    "           raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "      \n",
    "       new_passenger['sex'] = new_passenger['sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "       new_passenger['alone'] = new_passenger['alone'].apply(lambda x: 1 if x == True else 0)\n",
    "      \n",
    "       onehot = self.encoder.transform(new_passenger[['embarked']]).toarray()\n",
    "       cols = ['embarked_' + val for val in self.encoder.categories_[0]]\n",
    "       new_passenger[cols] = pd.DataFrame(onehot, index=new_passenger.index)\n",
    "       new_passenger.drop(['embarked'], axis=1, inplace=True)\n",
    "       new_passenger.drop(['name'], axis=1, inplace=True)\n",
    "      \n",
    "       dead_proba, alive_proba = np.squeeze(self.model_logreg.predict_proba(new_passenger))\n",
    "       print('Death probability: {:.2%}'.format(dead_proba)) \n",
    "       print('Survival probability: {:.2%}'.format(alive_proba)) \n",
    "       return dead_proba, alive_proba\n",
    "\n",
    "\n",
    "# Usage\n",
    "titanic_predictor = TitanicPredictor()\n",
    "titanic_predictor.load_data()\n",
    "titanic_predictor.preprocess_data()\n",
    "titanic_predictor.train_models()\n",
    "titanic_predictor.evaluate_models()\n",
    "\n",
    "\n",
    "# Define a new passenger\n",
    "passenger = pd.DataFrame({\n",
    "   'name': ['John Mortensen'],\n",
    "   'pclass': [2],\n",
    "   'sex': ['male'],\n",
    "   'age': [64],\n",
    "   'sibsp': [1],\n",
    "   'parch': [1],\n",
    "   'fare': [16.00],\n",
    "   'embarked': ['S'],\n",
    "   'alone': [False]\n",
    "})\n",
    "\n",
    "\n",
    "titanic_predictor.predict_survival_probability(passenger)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Model: \n",
    "\n",
    "Data Loading and Preprocessing:\n",
    "The model begins by loading the Titanic dataset using Seaborn's load_dataset() function.\n",
    "It then preprocesses the data by dropping irrelevant columns ('alive', 'who', 'adult_male', 'class', 'embark_town', 'deck') and handling missing values.\n",
    "Categorical variables like 'sex' and 'alone' are converted into numerical format.\n",
    "\n",
    "One-Hot Encoding:\n",
    "The model uses one-hot encoding to convert the categorical variable 'embarked' into binary vectors.\n",
    "\n",
    "Model Training:\n",
    "After preprocessing, the data is split into features (X) and the target variable (y), followed by splitting into training and testing sets.\n",
    "Two models are trained: a Decision Tree Classifier (model_dt) and a Logistic Regression model (model_logreg).\n",
    "\n",
    "Model Evaluation:\n",
    "The trained models are evaluated using accuracy scores on the test data.\n",
    "\n",
    "Prediction:\n",
    "The model provides a method predict_survival_probability() to predict the survival probability of a new passenger.\n",
    "The new passenger's data is preprocessed similarly to the training data.\n",
    "The survival probability is predicted using the trained Logistic Regression model.\n",
    "\n",
    "Usage Example:\n",
    "An instance of the TitanicPredictor class is created.\n",
    "Data is loaded, preprocessed, models are trained, and then evaluated.\n",
    "A new passenger's data is defined, and the predict_survival_probability() method is called to estimate their survival probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Blueprint, jsonify, request  # jsonify creates an endpoint response object\n",
    "from flask_restful import Api, Resource # used for REST API building\n",
    "\n",
    "from model.jokes import *\n",
    "\n",
    "joke_api = Blueprint('joke_api', __name__,\n",
    "                   url_prefix='/api/jokes')\n",
    "\n",
    "# API generator https://flask-restful.readthedocs.io/en/latest/api.html#id1\n",
    "api = Api(joke_api)\n",
    "\n",
    "class TitanicAPI(Resource):\n",
    "    def post(self):\n",
    "            # Get passenger data from the API request\n",
    "            data = request.get_json()  # get the data as JSON\n",
    "            data['alone'] = str(data['alone']).lower()\n",
    "            converted_dict = {key: [value] for key, value in data.items()}\n",
    "            pass_in = pd.DataFrame(converted_dict)  # create DataFrame from JSON\n",
    "            titanic_predictor = TitanicPredictor()\n",
    "            titanic_predictor.load_data()\n",
    "            titanic_predictor.preprocess_data()\n",
    "            titanic_predictor.train_models()\n",
    "            titanic_predictor.evaluate_models()\n",
    "            dead_proba, alive_proba = titanic_predictor.predict_survival_probability(pass_in)\n",
    "            response = {\n",
    "                'dead_proba': dead_proba,  # Example probabilities, replace with actual values\n",
    "                'alive_proba': alive_proba\n",
    "            }\n",
    "            return jsonify(response)\n",
    "\n",
    "\n",
    "# Add resource to the API\n",
    "api.add_resource(TitanicAPI, '/create')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic API:\n",
    "\n",
    "The TitanicAPI class is a Flask-Restful Resource representing an endpoint of the API. It handles POST requests to the /api/jokes/create endpoint.\n",
    "In the post method, it extracts passenger data from the JSON request using request.get_json().\n",
    "The passenger data is processed and formatted, and then passed to the TitanicPredictor class (assumed to be defined elsewhere) for prediction.\n",
    "After predicting survival probabilities, a JSON response containing the probabilities is created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPT ML (Depression):\n",
    "\n",
    "For our CPT project, we have decided to create a project centering around mental health and self care. We decided that we would try to find A dataset dealing with depression rates. We found a few we liked, and even turned created a dataset. We created a model to train data form the dataset to predict how likley a person would develop depression due to these factors: age, stress level, exercise, and sleep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('depression_dataset.csv')\n",
    "# Split the data into features and labels\n",
    "X = data.drop('Probability of Developing Depression', axis=1)\n",
    "y = data['Probability of Developing Depression']\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "# Function to predict the chance of being depressed\n",
    "def predict_depression(age, stress_level, exercise_hours, sleep_hours):\n",
    "    input_data = scaler.transform([[age, stress_level, exercise_hours, sleep_hours]])\n",
    "    chance_of_depression = model.predict(input_data)[0]\n",
    "    return chance_of_depression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CPT Model:\n",
    "\n",
    "Data Loading and Preparation:\n",
    "The model begins by loading a dataset containing pertinent information for predicting depression. This dataset typically comprises features such as age, stress level, exercise hours, and sleep hours, alongside a target variable indicating the likelihood of developing depression.\n",
    "Subsequently, the model divides the data into features (X) and the target variable (y). Features represent the input variables utilized for predictions, while the target variable signifies what we aim to predict—in this instance, the probability of experiencing depression.\n",
    "\n",
    "Data Preprocessing:\n",
    "Before proceeding with model training, it's imperative to preprocess the data. Within this model, data preprocessing entails standardization of features using a method known as StandardScaler. Standardization ensures that all features exhibit a mean of 0 and a standard deviation of 1, thereby enhancing the efficacy of certain machine learning algorithms.\n",
    "\n",
    "Model Training:\n",
    "The model undergoes training employing a linear regression algorithm for prediction tasks. Linear regression, although straightforward, is a robust algorithm employed for establishing relationships between a dependent variable (in this scenario, the likelihood of depression) and one or more independent variables (the features).\n",
    "Training involves utilizing the preprocessed training data (X_train, y_train). Throughout this process, the model learns to discern the relationship between the input features and the target variable by minimizing the disparity between predicted values and actual observations, a process known as minimizing the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class DepressionPredictor:\n",
    "    def __init__(self):\n",
    "        self.data = None\n",
    "        self.model_logreg = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        self.scaler = None\n",
    "      \n",
    "    def load_data(self, filepath):\n",
    "        self.data = pd.read_csv(filepath)\n",
    "      \n",
    "    def preprocess_data(self):\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "      \n",
    "        # Any necessary preprocessing steps can be added here\n",
    "\n",
    "        # For example, dropping columns, handling missing values, encoding categorical variables, etc.\n",
    "      \n",
    "    def train_models(self):\n",
    "        if self.data is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_data() first.\")\n",
    "        \n",
    "        X = self.data.drop('Depression', axis=1)\n",
    "        y = self.data['Depression']\n",
    "        X_train, self.X_test, y_train, self.y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "      \n",
    "        self.scaler = StandardScaler()\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "      \n",
    "        self.model_logreg = LogisticRegression()\n",
    "        self.model_logreg.fit(X_train_scaled, y_train)\n",
    "      \n",
    "    def evaluate_models(self):\n",
    "        if self.model_logreg is None:\n",
    "            raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "      \n",
    "        y_pred_logreg = self.model_logreg.predict(self.X_test)\n",
    "        accuracy_logreg = accuracy_score(self.y_test, y_pred_logreg)\n",
    "        print('LogisticRegression Accuracy: {:.2%}'.format(accuracy_logreg)) \n",
    "  \n",
    "    def predict_depression_probability(self, new_data):\n",
    "    if self.model_logreg is None:\n",
    "        raise ValueError(\"Models not trained. Call train_models() first.\")\n",
    "\n",
    "    # Preprocess new data similarly to training data\n",
    "    new_data_processed = new_data.copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    new_data_processed['Family History of Depression'] = new_data_processed['family_history'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
    "    # Add any additional preprocessing steps here\n",
    "\n",
    "    # Ensure consistency in feature names and order\n",
    "    new_data_processed = new_data_processed.rename(columns={'age': 'Age',\n",
    "                                                            'exercise_hours': 'Daily Exercise Hours',\n",
    "                                                            'family_history': 'Family History of Depression',\n",
    "                                                            'sleep_hours': 'Daily Sleep Hours',\n",
    "                                                            'stress_level': 'Stress Level'})\n",
    "\n",
    "    # Drop duplicate 'Family History of Depression' column if present\n",
    "    if 'Family History of Depression' in new_data_processed.columns:\n",
    "        new_data_processed = new_data_processed.drop('Family History of Depression', axis=1)\n",
    "\n",
    "    # Check if 'Probability of Developing Depression' column is present\n",
    "    if 'Probability of Developing Depression' in new_data_processed.columns:\n",
    "        new_data_processed = new_data_processed.drop('Probability of Developing Depression', axis=1)\n",
    "\n",
    "    # Debug: Print feature names in the new data\n",
    "    print(\"Feature names in new data:\", new_data_processed.columns)\n",
    "\n",
    "    # Ensure that feature names match those seen during fit time\n",
    "    expected_feature_names = set(['Age', 'Daily Exercise Hours', 'Family History of Depression', 'Daily Sleep Hours', 'Stress Level'])\n",
    "    new_feature_names = set(new_data_processed.columns)\n",
    "    if expected_feature_names != new_feature_names:\n",
    "        missing_features = expected_feature_names - new_feature_names\n",
    "        raise ValueError(f\"Feature names seen at fit time, yet now missing: {missing_features}\")\n",
    "\n",
    "    # Transform new data using the scaler\n",
    "    new_data_scaled = self.scaler.transform(new_data_processed)\n",
    "\n",
    "    # Predict the probability of depression\n",
    "    probability_of_depression = self.model_logreg.predict_proba(new_data_scaled)[:, 1]\n",
    "    return probability_of_depression\n",
    "\n",
    "# Usage\n",
    "depression_predictor = DepressionPredictor()\n",
    "depression_predictor.load_data('depression_dataset.csv')\n",
    "depression_predictor.preprocess_data()\n",
    "depression_predictor.train_models()\n",
    "depression_predictor.evaluate_models()\n",
    "\n",
    "# Define new data for prediction\n",
    "new_data = pd.DataFrame({\n",
    "    'age': [30],\n",
    "    'family_history': ['Yes'],  # Assuming 'Yes' or 'No' as values\n",
    "    'stress_level': [5],\n",
    "    'exercise_hours': [1.5],\n",
    "    'sleep_hours': [8]\n",
    "})\n",
    "\n",
    "probability_of_depression = depression_predictor.predict_depression_probability(new_data)\n",
    "print('Probability of depression:', probability_of_depression)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of a real dataset, we've found a few that we can use to train another model. One of the datasets provided was about students and their mwajors and how that would affect them mentally. Another one was about workers in the tech industry and how likley their circumstances would correlate with developing depression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
